{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e1381e7",
   "metadata": {},
   "source": [
    "# 🧠 LLM Model Compression & Evaluation Pipeline\n",
    "\n",
    "This Jupyter Notebook walks through a structured pipeline that automates the process of compressing large language models (LLMs) and evaluating their performance.\n",
    "\n",
    "## 🔧 What this Notebook Does:\n",
    "1. **Installs prerequisites**\n",
    "2. **Lists and selects available models**\n",
    "3. **Skips specified models for compression or evaluation**\n",
    "4. **Applies quantization techniques (e.g., INT8, INT4, FP16)**\n",
    "5. **Evaluates models using ROUGE, BLEU, and time metrics**\n",
    "6. **Generates a comparison report from evaluation CSVs**\n",
    "\n",
    "## 📦 Model Formats Supported:\n",
    "- INT8 Quantization\n",
    "- INT4 Quantization\n",
    "- FP16 Precision\n",
    "\n",
    "> 💡 Models and configurations are loaded from an external file (`llm_config.py`).\n",
    "\n",
    "## 📊 Evaluation Metrics:\n",
    "- **ROUGE Score** ([read more](https://en.wikipedia.org/wiki/ROUGE_(metric)))\n",
    "- **BLEU Score** ([read more](https://en.wikipedia.org/wiki/BLEU))\n",
    "- **Inference Time** (Average response time per model)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118cc45b",
   "metadata": {},
   "source": [
    "## 🔧 Step 1: Environment Setup\n",
    "\n",
    "The pipeline begins with the installation of all necessary libraries and setup of the working environment. \n",
    "\n",
    "If you're running this on a remote server or Colab, make sure the following steps execute successfully before continuing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8d56a8-cbce-4a25-af69-613b3da842b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steps: \n",
    "# 1. Install Prerequisites\n",
    "# 2. Get the Configuration File.\n",
    "# 3. Check available models\n",
    "# 4. Select models to skip for Compression\n",
    "# 5. Compress every models for every 3 variants\n",
    "# 6. Select models to skip for Evaluation\n",
    "# 7. Evaluate selected models per Compression Variant. \n",
    "# 8. Save the Results in a CSV File (Per Compression Variant)\n",
    "\n",
    "import os\n",
    "import platform\n",
    "\n",
    "os.environ[\"GIT_CLONE_PROTECTION_ACTIVE\"] = \"false\"\n",
    "\n",
    "#added installations \n",
    "%pip install rouge-score \n",
    "%pip install ipywidgets\n",
    "%pip install pyngrok\n",
    " \n",
    "#existing installations\n",
    "%pip install -Uq pip\n",
    "%pip uninstall -q -y optimum optimum-intel\n",
    "%pip install --pre -Uq \"openvino>=2024.2.0\" openvino-tokenizers[transformers] --extra-index-url https://storage.openvinotoolkit.org/simple/wheels/nightly\n",
    "%pip install -q --extra-index-url https://download.pytorch.org/whl/cpu\\\n",
    "\"git+https://github.com/huggingface/optimum-intel.git\"\\\n",
    "\"nncf==2.14.1\"\\\n",
    "\"torch>=2.1\"\\\n",
    "\"datasets\" \\\n",
    "\"accelerate\" \\\n",
    "\"gradio>=4.19\" \\\n",
    "\"huggingface-hub>=0.26.5\" \\\n",
    " \"einops\" \"transformers>=4.43.1\" \"transformers_stream_generator\" \"tiktoken\" \"bitsandbytes\"\n",
    "\n",
    "if platform.system() == \"Darwin\":\n",
    "    %pip install -q \"numpy<2.0.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b92b01",
   "metadata": {},
   "source": [
    "## ⚙️ Step 2: Model Configuration\n",
    "\n",
    "This step fetches and prepares the model configuration file, which includes metadata for each LLM to be evaluated or compressed. The configuration is pulled from a central location or a locally-defined file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edacadf0-a01a-42ce-8026-650cad845c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import shutil\n",
    "\n",
    "# fetch model configuration\n",
    "\n",
    "config_shared_path = Path(\"../../utils/llm_config.py\")\n",
    "config_dst_path = Path(\"llm_config.py\")\n",
    "\n",
    "if not config_dst_path.exists():\n",
    "    if config_shared_path.exists():\n",
    "        try:\n",
    "            os.symlink(config_shared_path, config_dst_path)\n",
    "        except Exception:\n",
    "            shutil.copy(config_shared_path, config_dst_path)\n",
    "    else:\n",
    "        r = requests.get(url=\"https://raw.githubusercontent.com/GodreignElgin/llm-comparision/llm_config.py\")\n",
    "        with open(\"llm_config.py\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(r.text)\n",
    "elif not os.path.islink(config_dst_path):\n",
    "    print(\"LLM config will be updated\")\n",
    "    if config_shared_path.exists():\n",
    "        shutil.copy(config_shared_path, config_dst_path)\n",
    "    else:\n",
    "        r = requests.get(url=\"https://raw.githubusercontent.com/GodreignElgin/llm-comparision/llm_config.py\")\n",
    "        with open(\"llm_config.py\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2d3151",
   "metadata": {},
   "source": [
    "## 📦 Step 3: Import Model Support Info\n",
    "\n",
    "Here, the notebook loads a list of supported models (`SUPPORTED_LLM_MODELS`) from an external Python script. It also sets up necessary interactive widgets for later UI-based model selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb569f6-1f41-438a-8630-995b8235475c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_config import SUPPORTED_LLM_MODELS\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3f66a2",
   "metadata": {},
   "source": [
    "## 🧠 Check System Resources\n",
    "\n",
    "To avoid memory issues during evaluation, this cell checks for available RAM. For memory-intensive models, having at least **16–32 GB** of RAM is ideal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab02c330-5f67-4d37-bdb9-2f3674b64ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to check the current available RAM memory before doing evalation\n",
    "import psutil\n",
    "print(f\"Available Memory: {psutil.virtual_memory().available / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfc541d",
   "metadata": {},
   "source": [
    "## 📋 List All Available Models\n",
    "\n",
    "You’ll now see a list of all the models configured for compression/evaluation. Each model name here can be selected or skipped based on your requirement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc26b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To list all the available Models. \n",
    "# The model name listed in below this is the model name you should give in the \n",
    "models = list(SUPPORTED_LLM_MODELS)\n",
    "\n",
    "models = widgets.Dropdown(\n",
    "    options=models,\n",
    "    value=models[0],\n",
    "    description=\"Models Available: \",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b06824",
   "metadata": {},
   "source": [
    "## ⛔ Optional: Skip Models for Compression\n",
    "\n",
    "You may not want to compress all models—especially if they've already been processed. This step uses a widget-based multi-select to allow skipping specific models for **compression only**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9b68af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the models to skip for Compression (all variants)\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "models = SUPPORTED_LLM_MODELS\n",
    "checkboxes = [widgets.Checkbox(value=False, description=model) for model in models]\n",
    "submit_button = widgets.Button(description=\"Submit\")\n",
    "output = widgets.Output()\n",
    "\n",
    "SKIP_MODELS_COMPRESSION = []\n",
    "\n",
    "def on_submit_clicked(b):\n",
    "    SKIP_MODELS_COMPRESSION = [cb.description for cb in checkboxes if cb.value]\n",
    "    with output:\n",
    "        clear_output()\n",
    "        print(\"Skipped models for Compression: \", SKIP_MODELS_COMPRESSION)\n",
    "\n",
    "submit_button.on_click(on_submit_clicked)\n",
    "\n",
    "display(widgets.VBox(checkboxes + [submit_button, output]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0696a4c3",
   "metadata": {},
   "source": [
    "## 🔁 Iterate Through Compressible Models\n",
    "\n",
    "This cell prepares a loop through the models that **aren’t skipped**, applying quantization methods such as:\n",
    "- [INT8](https://huggingface.co/docs/transformers/performance#dynamic-quantization)\n",
    "- [INT4](https://huggingface.co/docs/optimum/intel/quantization)\n",
    "- [FP16](https://huggingface.co/docs/transformers/performance#mixed-precision-training)\n",
    "\n",
    "You can view how models are compressed internally in the logs printed during execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83477d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through all the available models except ones mentioned in the SKIP_MODELS_COMPRESSION\n",
    "# to compress the models and store it automatically.\n",
    "# The Compressions done in this code are FP16, INT8 and INT4. To include other versions, you need to explicitly write code for that.\n",
    "from pathlib import Path\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Iterate through all supported models except the ones in SKIP_MODELS\n",
    "for model_name in SUPPORTED_LLM_MODELS:\n",
    "    if model_name in SKIP_MODELS_COMPRESSION:\n",
    "        print(f\"Skipping model: {model_name}\")\n",
    "        continue\n",
    "    \n",
    "    model_configuration = SUPPORTED_LLM_MODELS[model_name]\n",
    "    print(f\"Processing model: {model_name}\")\n",
    "    \n",
    "    pt_model_id = model_configuration[\"model_id\"]\n",
    "    fp16_model_dir = Path(model_name) / \"FP16\"\n",
    "    int8_model_dir = Path(model_name) / \"INT8\"\n",
    "    int4_model_dir = Path(model_name) / \"INT4\"\n",
    "\n",
    "    def convert_to_fp16():\n",
    "        if not (fp16_model_dir / \"openvino_model.xml\").exists():\n",
    "            remote_code = model_configuration.get(\"remote_code\", False)\n",
    "            export_command = f\"optimum-cli export openvino --model {pt_model_id} --task text-generation-with-past --weight-format fp16 {str(fp16_model_dir)}\"\n",
    "            if remote_code:\n",
    "                export_command += \" --trust-remote-code\"\n",
    "            display(Markdown(f\"**Export command:** {export_command}\"))\n",
    "            ! $export_command\n",
    "\n",
    "    def convert_to_int8():\n",
    "        if not (int8_model_dir / \"openvino_model.xml\").exists():\n",
    "            int8_model_dir.mkdir(parents=True, exist_ok=True)\n",
    "            remote_code = model_configuration.get(\"remote_code\", False)\n",
    "            export_command = f\"optimum-cli export openvino --model {pt_model_id} --task text-generation-with-past --weight-format int8 {str(int8_model_dir)}\"\n",
    "            if remote_code:\n",
    "                export_command += \" --trust-remote-code\"\n",
    "            display(Markdown(f\"**Export command:** {export_command}\"))\n",
    "            ! $export_command\n",
    "\n",
    "    def convert_to_int4():\n",
    "        compression_configs = {\n",
    "            \"default\": {\"sym\": False, \"group_size\": 128, \"ratio\": 0.8},\n",
    "            \"zephyr-7b-beta\": {\"sym\": True, \"group_size\": 64, \"ratio\": 0.6},\n",
    "            \"mistral-7b\": {\"sym\": True, \"group_size\": 64, \"ratio\": 0.6},\n",
    "            \"minicpm-2b-dpo\": {\"sym\": True, \"group_size\": 64, \"ratio\": 0.6},\n",
    "            \"gemma-2b-it\": {\"sym\": True, \"group_size\": 64, \"ratio\": 0.6},\n",
    "            \"notus-7b-v1\": {\"sym\": True, \"group_size\": 64, \"ratio\": 0.6},\n",
    "            \"neural-chat-7b-v3-1\": {\"sym\": True, \"group_size\": 64, \"ratio\": 0.6},\n",
    "            \"llama-2-chat-7b\": {\"sym\": True, \"group_size\": 128, \"ratio\": 0.8},\n",
    "            \"llama-3-8b-instruct\": {\"sym\": True, \"group_size\": 128, \"ratio\": 0.8},\n",
    "            \"llama-3.1-8b-instruct\": {\"sym\": True, \"group_size\": 128, \"ratio\": 1.0},\n",
    "            \"gemma-7b-it\": {\"sym\": True, \"group_size\": 128, \"ratio\": 0.8},\n",
    "            \"chatglm2-6b\": {\"sym\": True, \"group_size\": 128, \"ratio\": 0.72},\n",
    "            \"qwen-7b-chat\": {\"sym\": True, \"group_size\": 128, \"ratio\": 0.6},\n",
    "            \"red-pajama-3b-chat\": {\"sym\": False, \"group_size\": 128, \"ratio\": 0.5},\n",
    "            \"qwen2.5-7b-instruct\": {\"sym\": True, \"group_size\": 128, \"ratio\": 1.0},\n",
    "            \"qwen2.5-3b-instruct\": {\"sym\": True, \"group_size\": 128, \"ratio\": 1.0},\n",
    "            \"qwen2.5-14b-instruct\": {\"sym\": True, \"group_size\": 128, \"ratio\": 1.0},\n",
    "            \"qwen2.5-1.5b-instruct\": {\"sym\": True, \"group_size\": 128, \"ratio\": 1.0},\n",
    "            \"qwen2.5-0.5b-instruct\": {\"sym\": True, \"group_size\": 128, \"ratio\": 1.0},\n",
    "        }\n",
    "        model_compression_params = compression_configs.get(model_name, compression_configs[\"default\"])\n",
    "\n",
    "        if not (int4_model_dir / \"openvino_model.xml\").exists():\n",
    "            remote_code = model_configuration.get(\"remote_code\", False)\n",
    "            export_command = f\"optimum-cli export openvino --model {pt_model_id} --task text-generation-with-past --weight-format int4\"\n",
    "            export_command += f\" --group-size {model_compression_params['group_size']} --ratio {model_compression_params['ratio']}\"\n",
    "            if model_compression_params[\"sym\"]:\n",
    "                export_command += \" --sym\"\n",
    "            export_command += f\" {str(int4_model_dir)}\"\n",
    "            if remote_code:\n",
    "                export_command += \" --trust-remote-code\"\n",
    "            display(Markdown(f\"**Export command:** {export_command}\"))\n",
    "            ! $export_command\n",
    "\n",
    "    # Convert models if needed\n",
    "    convert_to_fp16()\n",
    "    convert_to_int8()\n",
    "    convert_to_int4()\n",
    "\n",
    "    print(f\"Finished processing {model_name}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae7d783",
   "metadata": {},
   "source": [
    "## ⛔ Optional: Skip Models for Evaluation\n",
    "\n",
    "Similar to compression, this allows skipping models from the **evaluation** stage using another widget selector. Skipping is helpful when running tests iteratively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298d3b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the models to skip for Evaluation (all variants)\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "models = SUPPORTED_LLM_MODELS\n",
    "checkboxes = [widgets.Checkbox(value=False, description=model) for model in models]\n",
    "submit_button = widgets.Button(description=\"Submit\")\n",
    "output = widgets.Output()\n",
    "\n",
    "SKIP_MODELS_EVALUATION = []\n",
    "\n",
    "def on_submit_clicked(b):\n",
    "    SKIP_MODELS_EVALUATION = [cb.description for cb in checkboxes if cb.value]\n",
    "    with output:\n",
    "        clear_output()\n",
    "        print(\"Skipped models for Evaluation: \", SKIP_MODELS_EVALUATION)\n",
    "\n",
    "submit_button.on_click(on_submit_clicked)\n",
    "\n",
    "display(widgets.VBox(checkboxes + [submit_button, output]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abaf8bc2",
   "metadata": {},
   "source": [
    "# Evaluation Pipeline \n",
    "\n",
    "This Pipeline code is designed to iterate through all the available models from the SUPPORTED_LLM_MODELS list, and Evaluate on 11 Evaluation Metrics. The Evaluation results are Stored in a CSV File for future reference. The code is well-structured, readable, and follows best practices.\n",
    "\n",
    "This pipeline runs through only the specified Compression.\n",
    "\n",
    "## 🧪 Model Evaluation Setup\n",
    "\n",
    "Now that the compression configuration is set up, we begin loading required libraries and initializing tools for evaluation.\n",
    "\n",
    "This includes:\n",
    "- Tokenizers from 🤗 Hugging Face,\n",
    "- Optimum libraries for quantized model loading,\n",
    "- Metrics libraries for BLEU/ROUGE.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42c6eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.translate.chrf_score import corpus_chrf\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def evaluate_model(model, tokenizer, input_text, reference_texts):\n",
    "    # Time stamp \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Running the model with the sample input text.  \n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "    output_ids = model.generate(input_ids=input_ids, max_new_tokens=128)\n",
    "    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Latency calculation\n",
    "    latency = (time.time() - start_time) * 1000\n",
    "    num_tokens = len(output_ids[0])\n",
    "\n",
    "    # Throughput calculation\n",
    "    throughput = num_tokens / (latency / 1000)\n",
    "    \n",
    "    # Rouge Score Calcuator\n",
    "    scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
    "    rouge_scores = scorer.score(reference_texts, generated_text)\n",
    "    \n",
    "    # BLEU Score Calculator\n",
    "    smoothing = SmoothingFunction().method1\n",
    "    bleu_score = sentence_bleu([reference_texts.split()], generated_text.split(), smoothing_function=smoothing)\n",
    "    \n",
    "    # chrf Score Calculator\n",
    "    chrf_score = corpus_chrf([[reference_texts]], [[generated_text]])\n",
    "    \n",
    "    # Unique ngrams, entropy and repeated ngrams Calculation\n",
    "    tokens = generated_text.split()\n",
    "    unique_ngrams = len(set(zip(tokens, tokens[1:]))) / len(tokens) if len(tokens) > 1 else 0\n",
    "    entropy = -np.sum([p * np.log2(p) for p in np.unique(tokens, return_counts=True)[1] / len(tokens)])\n",
    "    repeated_ngrams = sum([1 for i in range(len(tokens) - 1) if tokens[i] == tokens[i + 1]]) / len(tokens)\n",
    "    \n",
    "    try:\n",
    "        ref_emb = model.get_input_embeddings()(input_ids).detach().numpy()\n",
    "        gen_emb = model.get_input_embeddings()(output_ids).detach().numpy()\n",
    "\n",
    "        #Coherence Calculator.\n",
    "        coherence = cosine_similarity(ref_emb.mean(axis=1), gen_emb.mean(axis=1))[0][0]\n",
    "    except Exception:\n",
    "        coherence = 0\n",
    "    \n",
    "    return {\n",
    "        \"Model\": f\"{model.config.name_or_path}_{compression_dir}\",\n",
    "        \"Latency (ms)\": latency,\n",
    "        \"Throughput (tokens/sec)\": throughput,\n",
    "        \"ROUGE-1\": rouge_scores[\"rouge1\"].fmeasure,\n",
    "        \"ROUGE-2\": rouge_scores[\"rouge2\"].fmeasure,\n",
    "        \"ROUGE-L\": rouge_scores[\"rougeL\"].fmeasure,\n",
    "        \"BLEU Score\": bleu_score,\n",
    "        \"CHRF Score\": chrf_score,\n",
    "        \"Unique n-grams\": unique_ngrams,\n",
    "        \"Entropy\": entropy,\n",
    "        \"Repeated n-grams (%)\": repeated_ngrams * 100,\n",
    "        \"Coherence (Cosine Similarity)\": coherence,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881a3e86",
   "metadata": {},
   "source": [
    "## 🧊 Model Compression Pipeline\n",
    "\n",
    "For each model:\n",
    "1. Loads the base version using the Hugging Face `AutoModelForCausalLM` class.\n",
    "2. Applies selected quantization format (FP16 / INT8 / INT4).\n",
    "3. Saves the quantized model in the appropriate format.\n",
    "\n",
    "🔧 Behind the scenes, it uses:\n",
    "- `optimum.intel` for INT8\n",
    "- `bitsandbytes` for INT4 (if available)\n",
    "\n",
    "📎 [Learn about model quantization](https://huggingface.co/docs/transformers/performance#model-quantization)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144007ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from transformers import AutoConfig, AutoTokenizer\n",
    "from optimum.intel.openvino import OVModelForCausalLM\n",
    "import openvino.properties as props\n",
    "import openvino.properties.hint as hints\n",
    "import openvino.properties.streams as streams\n",
    "\n",
    "def evaluate_models_and_save(SUPPORTED_LLM_MODELS, compression_dir):\n",
    "\n",
    "    output_csv=f\"evaluation_results_{compression_dir}.csv\"\n",
    "\n",
    "    # results stored as a list\n",
    "    all_results = []\n",
    "    existing_df = None\n",
    "    \n",
    "    # check if the file already exists, to append to the file and not accidentally create new file everytime and overwrite over it.\n",
    "    if Path(output_csv).exists():\n",
    "        existing_df = pd.read_csv(output_csv)\n",
    "    \n",
    "    for model_name, model_configuration in SUPPORTED_LLM_MODELS.items():\n",
    "\n",
    "        if model_name in SKIP_MODELS_EVALUATION:\n",
    "            print(f\"Skipping model: {model_name}\")\n",
    "            continue\n",
    "        print(f\"Processing model: {model_name}\")\n",
    "        \n",
    "        #loads only the specified precision model file. \n",
    "        model_dir = Path(model_name) / compression_dir  \n",
    "\n",
    "        # only loads the openvino format model. \n",
    "        if not (model_dir / \"openvino_model.xml\").exists():\n",
    "            continue\n",
    "        \n",
    "        print(f\"Loading model from {model_dir}\")\n",
    "        ov_config = {\n",
    "            hints.performance_mode(): hints.PerformanceMode.THROUGHPUT,\n",
    "            streams.num(): \"AUTO\",\n",
    "            props.cache_dir(): \"ov_cache\"\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\n",
    "            model = OVModelForCausalLM.from_pretrained(\n",
    "                model_dir,\n",
    "                device=\"CPU\", # u can set the device here. GPU if u have. \n",
    "                ov_config=ov_config,\n",
    "                config=AutoConfig.from_pretrained(model_dir, trust_remote_code=True),\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {model_name}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # sample input text for testing. \n",
    "        input_text = \"2 + 2 =\"\n",
    "\n",
    "        # one-shot testing (simple one shot)\n",
    "        reference_texts = \"2 + 2 = 4\"\n",
    "        \n",
    "        try:\n",
    "            results = evaluate_model(model, tokenizer, input_text, reference_texts)\n",
    "            results[\"Model\"] = model_name\n",
    "            all_results.append(results)\n",
    "        except Exception as e:\n",
    "            print(f\"Error during evaluation of {model_name}: {e}\")\n",
    "            \n",
    "        # Deletes the evaluated model and its assigned tokenizer to save RAM memory. \n",
    "        del model, tokenizer\n",
    "\n",
    "        # Initializes the Garbage collector to free up memory.\n",
    "        gc.collect()\n",
    "    \n",
    "    # Adding the results to a dataframe for easier analysis.\n",
    "    df = pd.DataFrame(all_results)\n",
    "    if existing_df is not None:\n",
    "        df = pd.concat([existing_df, df], ignore_index=True)\n",
    "    \n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"Results appended to {output_csv}\")\n",
    "\n",
    "compression_dirs = [\"FP16\", \"INT8\", \"INT4\"]\n",
    "# make sure to set the compression_dir vairable\n",
    "# compression_dir = model_compressed_directory_name \n",
    "# Example, compression_dir = FP16 (if the directory is named FP16 in the format, model/FP16/the compressed files.)\n",
    "for compression_dir in compression_dirs:\n",
    "    evaluate_models_and_save(SUPPORTED_LLM_MODELS, compression_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fea694",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the int4 data\n",
    "data_int4 = \"\"\"Model,Latency (ms),Throughput (tokens/sec),ROUGE-1,ROUGE-2,ROUGE-L,BLEU Score,CHRF Score,Unique n-grams,Entropy,Repeated n-grams (%),Coherence (Cosine Similarity)\n",
    "qwen2.5-0.5b-instruct,3718.015432357788,35.77177190888036,0.0789473684210526,0.054054054054054,0.0789473684210526,0.0339483954244015,0.0813940147091148,0.696969696969697,5.182563191622859,0.0,0.0\n",
    "tiny-llama-1b-chat,4734.71999168396,28.51277377270744,0.074074074074074,0.050632911392405,0.074074074074074,0.0421667093080309,0.0785752665303753,0.55,4.512492001110319,1.25,0.0\n",
    "DeepSeek-R1-Distill-Qwen-1.5B,7707.976341247559,17.384588907328133,0.1428571428571428,0.05,0.1428571428571428,0.0503446068227303,0.1406866456493381,0.9791666666666666,4.634761657503722,0.0,0.0\n",
    "DeepSeek-R1-Distill-Qwen-7B,56090.147495269775,2.389011368017896,0.0681818181818181,0.0465116279069767,0.0681818181818181,0.0387138472888192,0.0682625017764336,0.8160919540229885,5.350315334630777,3.4482758620689653,0.0\n",
    "qwen2.5-1.5b-instruct,3660.7508659362793,9.834048073302192,0.3529411764705882,0.2666666666666667,0.3529411764705882,0.2140909265975804,0.2989303567713718,0.9411764705882352,3.734521664779752,0.0,0.0\n",
    "gemma-2b-it,4337.922811508179,7.837852695257876,0.2352941176470588,0.1333333333333333,0.2352941176470588,0.1431712315455507,0.2166282977428487,0.7647058823529411,3.337175341123077,0.0,0.0\n",
    "gemma-2-2b-it,4597.316265106201,7.395619104576564,0.2608695652173913,0.1904761904761905,0.2608695652173913,0.1700107809840422,0.2500253190521059,0.9523809523809524,4.297079327540666,0.0,0.0\n",
    "qwen2.5-3b-instruct,6167.704343795776,3.242697588142081,0.5,0.3333333333333333,0.5,0.2984745896009823,0.3151663151663151,0.8888888888888888,2.725480556997868,0.0,0.0\n",
    "DeepSeek-R1-Distill-Llama-8B,29102.829933166504,4.604363228858695,0.0681818181818181,0.0465116279069767,0.0681818181818181,0.0255901590405087,0.0657462943708585,0.8863636363636364,5.473580624294531,1.1363636363636365,0.0\n",
    "qwen2.5-7b-instruct,28736.822605133057,4.628208268795978,0.0615384615384615,0.0317460317460317,0.0615384615384615,0.0278438083263775,0.0709180336599263,0.8888888888888888,5.333875392324575,0.0,0.0\n",
    "llama-3.2-1b-instruct,5683.32839012146,23.5777331172546,0.1132075471698113,0.0784313725490195,0.1132075471698113,0.0401218776374591,0.188787574107971,0.5833333333333334,3.61007166776166,14.285714285714285,0.0\n",
    "llama-3.2-3b-instruct,13235.87131500244,10.12400293195018,0.0517241379310344,0.0350877192982456,0.0517241379310344,0.0291622159772622,0.0655736528349281,0.2608695652173913,3.50282594697423,0.0,0.0\n",
    "zephyr-7b-beta,30112.271785736084,4.483222021924905,0.0666666666666666,0.0454545454545454,0.0666666666666666,0.0241915748247312,0.0634091809588424,0.935483870967742,5.806481762856229,0.0,0.0\n",
    "gemma-2-9b-it,17309.56268310547,2.19531831599009,0.2222222222222222,0.16,0.2222222222222222,0.1410002457876886,0.21321465960841668,0.96,4.563856189774725,0.0,0.0\n",
    "\"\"\"\n",
    "from io import StringIO\n",
    "df_int4 = pd.read_csv(StringIO(data_int4))\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "print(\"First few rows of int4 data:\")\n",
    "print(df_int4.head())\n",
    "print(\"\\n\")\n",
    "\n",
    "# Basic information about the dataframe\n",
    "print(\"Information about int4 data:\")\n",
    "print(df_int4.info())\n",
    "print(\"\\n\")\n",
    "\n",
    "# 1. Latency vs. Throughput Scatter Plot (int4)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='Latency (ms)', y='Throughput (tokens/sec)', data=df_int4, hue='Model', s=100)\n",
    "plt.title('Latency vs. Throughput for INT4 Compressed Models')\n",
    "plt.xlabel('Latency (ms)')\n",
    "plt.ylabel('Throughput (tokens/sec)')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. Bar Chart for ROUGE-L (int4)\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='Model', y='ROUGE-L', data=df_int4, palette='viridis')\n",
    "plt.title('ROUGE-L Scores for INT4 Compressed Models')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('ROUGE-L Score')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. Bar Chart for BLEU Score (int4)\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='Model', y='BLEU Score', data=df_int4, palette='mako')\n",
    "plt.title('BLEU Scores for INT4 Compressed Models')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('BLEU Score')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
