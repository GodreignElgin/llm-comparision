{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8d56a8-cbce-4a25-af69-613b3da842b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steps: \n",
    "# 1. Install Prerequisites\n",
    "# 2. Get the Configuration File.\n",
    "# 3. Check available models\n",
    "# 4. Select model for Compression\n",
    "# 5. Compress model for the 3 variants\n",
    "# 6. Select model for Evaluation\n",
    "# 7. Evaluate selected model for all the Compression Variants. \n",
    "# 8. Save the Results in a CSV File (Per Compression Variant)\n",
    "\n",
    "import os\n",
    "import platform\n",
    "\n",
    "os.environ[\"GIT_CLONE_PROTECTION_ACTIVE\"] = \"false\"\n",
    "\n",
    "#added installations \n",
    "%pip install rouge-score \n",
    "%pip install ipywidgets\n",
    "%pip install pyngrok\n",
    " \n",
    "#existing installations\n",
    "%pip install -Uq pip\n",
    "%pip uninstall -q -y optimum optimum-intel\n",
    "%pip install --pre -Uq \"openvino>=2024.2.0\" openvino-tokenizers[transformers] --extra-index-url https://storage.openvinotoolkit.org/simple/wheels/nightly\n",
    "%pip install -q --extra-index-url https://download.pytorch.org/whl/cpu\\\n",
    "\"git+https://github.com/huggingface/optimum-intel.git\"\\\n",
    "\"nncf==2.14.1\"\\\n",
    "\"torch>=2.1\"\\\n",
    "\"datasets\" \\\n",
    "\"accelerate\" \\\n",
    "\"gradio>=4.19\" \\\n",
    "\"huggingface-hub>=0.26.5\" \\\n",
    " \"einops\" \"transformers>=4.43.1\" \"transformers_stream_generator\" \"tiktoken\" \"bitsandbytes\"\n",
    "\n",
    "if platform.system() == \"Darwin\":\n",
    "    %pip install -q \"numpy<2.0.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edacadf0-a01a-42ce-8026-650cad845c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import shutil\n",
    "\n",
    "# fetch model configuration\n",
    "\n",
    "config_shared_path = Path(\"../../utils/llm_config.py\")\n",
    "config_dst_path = Path(\"llm_config.py\")\n",
    "\n",
    "if not config_dst_path.exists():\n",
    "    if config_shared_path.exists():\n",
    "        try:\n",
    "            os.symlink(config_shared_path, config_dst_path)\n",
    "        except Exception:\n",
    "            shutil.copy(config_shared_path, config_dst_path)\n",
    "    else:\n",
    "        r = requests.get(url=\"https://raw.githubusercontent.com/GodreignElgin/llm-comparision/llm_config.py\")\n",
    "        with open(\"llm_config.py\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(r.text)\n",
    "elif not os.path.islink(config_dst_path):\n",
    "    print(\"LLM config will be updated\")\n",
    "    if config_shared_path.exists():\n",
    "        shutil.copy(config_shared_path, config_dst_path)\n",
    "    else:\n",
    "        r = requests.get(url=\"https://raw.githubusercontent.com/GodreignElgin/llm-comparision/llm_config.py\")\n",
    "        with open(\"llm_config.py\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb569f6-1f41-438a-8630-995b8235475c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_config import SUPPORTED_LLM_MODELS\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab02c330-5f67-4d37-bdb9-2f3674b64ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Memory: 0.70 GB\n"
     ]
    }
   ],
   "source": [
    "# to check the current available RAM memory before doing evalation\n",
    "import psutil\n",
    "print(f\"Available Memory: {psutil.virtual_memory().available / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc26b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qwen2.5-0.5b-instruct\n"
     ]
    }
   ],
   "source": [
    "# To list all the available Models. \n",
    "# The model name listed in below this is the model name you should give in the \n",
    "models = list(SUPPORTED_LLM_MODELS)\n",
    "\n",
    "models = widgets.Dropdown(\n",
    "    options=models,\n",
    "    value=models[0],\n",
    "    description=\"Models Available: \",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83477d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compress the Selected model into all three variants.\n",
    "# The Compressions done in this code are FP16, INT8 and INT4. To include other versions, you need to explicitly write code for that.\n",
    "from pathlib import Path\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "model_name = models.value\n",
    "model_configuration = SUPPORTED_LLM_MODELS[model_name]\n",
    "print(f\"Processing model: {model_name}\")\n",
    "    \n",
    "pt_model_id = model_configuration[\"model_id\"]\n",
    "fp16_model_dir = Path(model_name) / \"FP16\"\n",
    "int8_model_dir = Path(model_name) / \"INT8\"\n",
    "int4_model_dir = Path(model_name) / \"INT4\"\n",
    "\n",
    "def convert_to_fp16():\n",
    "    if not (fp16_model_dir / \"openvino_model.xml\").exists():\n",
    "        remote_code = model_configuration.get(\"remote_code\", False)\n",
    "        export_command = f\"optimum-cli export openvino --model {pt_model_id} --task text-generation-with-past --weight-format fp16 {str(fp16_model_dir)}\"\n",
    "        if remote_code:\n",
    "            export_command += \" --trust-remote-code\"\n",
    "        display(Markdown(f\"**Export command:** {export_command}\"))\n",
    "        ! $export_command\n",
    "\n",
    "def convert_to_int8():\n",
    "    if not (int8_model_dir / \"openvino_model.xml\").exists():\n",
    "        int8_model_dir.mkdir(parents=True, exist_ok=True)\n",
    "        remote_code = model_configuration.get(\"remote_code\", False)\n",
    "        export_command = f\"optimum-cli export openvino --model {pt_model_id} --task text-generation-with-past --weight-format int8 {str(int8_model_dir)}\"\n",
    "        if remote_code:\n",
    "            export_command += \" --trust-remote-code\"\n",
    "        display(Markdown(f\"**Export command:** {export_command}\"))\n",
    "        ! $export_command\n",
    "\n",
    "def convert_to_int4():\n",
    "    compression_configs = {\n",
    "        \"default\": {\"sym\": False, \"group_size\": 128, \"ratio\": 0.8},\n",
    "        \"zephyr-7b-beta\": {\"sym\": True, \"group_size\": 64, \"ratio\": 0.6},\n",
    "        \"mistral-7b\": {\"sym\": True, \"group_size\": 64, \"ratio\": 0.6},\n",
    "        \"minicpm-2b-dpo\": {\"sym\": True, \"group_size\": 64, \"ratio\": 0.6},\n",
    "        \"gemma-2b-it\": {\"sym\": True, \"group_size\": 64, \"ratio\": 0.6},\n",
    "        \"notus-7b-v1\": {\"sym\": True, \"group_size\": 64, \"ratio\": 0.6},\n",
    "        \"neural-chat-7b-v3-1\": {\"sym\": True, \"group_size\": 64, \"ratio\": 0.6},\n",
    "        \"llama-2-chat-7b\": {\"sym\": True, \"group_size\": 128, \"ratio\": 0.8},\n",
    "        \"llama-3-8b-instruct\": {\"sym\": True, \"group_size\": 128, \"ratio\": 0.8},\n",
    "        \"llama-3.1-8b-instruct\": {\"sym\": True, \"group_size\": 128, \"ratio\": 1.0},\n",
    "        \"gemma-7b-it\": {\"sym\": True, \"group_size\": 128, \"ratio\": 0.8},\n",
    "        \"chatglm2-6b\": {\"sym\": True, \"group_size\": 128, \"ratio\": 0.72},\n",
    "        \"qwen-7b-chat\": {\"sym\": True, \"group_size\": 128, \"ratio\": 0.6},\n",
    "        \"red-pajama-3b-chat\": {\"sym\": False, \"group_size\": 128, \"ratio\": 0.5},\n",
    "        \"qwen2.5-7b-instruct\": {\"sym\": True, \"group_size\": 128, \"ratio\": 1.0},\n",
    "        \"qwen2.5-3b-instruct\": {\"sym\": True, \"group_size\": 128, \"ratio\": 1.0},\n",
    "        \"qwen2.5-14b-instruct\": {\"sym\": True, \"group_size\": 128, \"ratio\": 1.0},\n",
    "        \"qwen2.5-1.5b-instruct\": {\"sym\": True, \"group_size\": 128, \"ratio\": 1.0},\n",
    "        \"qwen2.5-0.5b-instruct\": {\"sym\": True, \"group_size\": 128, \"ratio\": 1.0},\n",
    "    }\n",
    "    model_compression_params = compression_configs.get(model_name, compression_configs[\"default\"])\n",
    "\n",
    "    if not (int4_model_dir / \"openvino_model.xml\").exists():\n",
    "        remote_code = model_configuration.get(\"remote_code\", False)\n",
    "        export_command = f\"optimum-cli export openvino --model {pt_model_id} --task text-generation-with-past --weight-format int4\"\n",
    "        export_command += f\" --group-size {model_compression_params['group_size']} --ratio {model_compression_params['ratio']}\"\n",
    "        if model_compression_params[\"sym\"]:\n",
    "            export_command += \" --sym\"\n",
    "        export_command += f\" {str(int4_model_dir)}\"\n",
    "        if remote_code:\n",
    "            export_command += \" --trust-remote-code\"\n",
    "        display(Markdown(f\"**Export command:** {export_command}\"))\n",
    "        ! $export_command\n",
    "# Convert models if needed\n",
    "convert_to_fp16()\n",
    "convert_to_int8()\n",
    "convert_to_int4()\n",
    "print(f\"Finished processing {model_name}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abaf8bc2",
   "metadata": {},
   "source": [
    "# Evaluation Pipeline \n",
    "\n",
    "This Pipeline code is designed to iterate through all the available models from the SUPPORTED_LLM_MODELS list, and Evaluate on 11 Evaluation Metrics. The Evaluation results are Stored in a CSV File for future reference. The code is well-structured, readable, and follows best practices.\n",
    "\n",
    "This pipeline runs through only the specified Compression.\n",
    "\n",
    "CHATGPT_ i need you to add steps of what all is happening in the below cell of code. put the steps in this markdown. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57ebd9b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['qwen2.5-0.5b-instruct', 'tiny-llama-1b-chat', 'DeepSeek-R1-Distill-Qwen-1.5B', 'DeepSeek-R1-Distill-Qwen-7B', 'DeepSeek-R1-Distill-Llama-8B', 'llama-3.2-1b-instruct', 'llama-3.2-3b-instruct', 'qwen2.5-1.5b-instruct', 'gemma-2b-it', 'gemma-2-2b-it', 'red-pajama-3b-chat', 'qwen2.5-3b-instruct', 'minicpm3-4b', 'qwen2.5-7b-instruct', 'gemma-7b-it', 'gemma-2-9b-it', 'llama-2-chat-7b', 'llama-3-8b-instruct', 'llama-3.1-8b-instruct', 'mistral-7b-instruct', 'zephyr-7b-beta', 'notus-7b-v1', 'neural-chat-7b-v3-3', 'phi-3-mini-instruct', 'phi-3.5-mini-instruct', 'phi-4-mini-instruct', 'phi-4', 'qwen2.5-14b-instruct']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad3a7ecb3811409e9103a974ab953dcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Models Available: ', options=('qwen2.5-0.5b-instruct', 'tiny-llama-1b-chat', 'DeepSeek-R…"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To list all the available Models. \n",
    "# The model name listed in below this is the model name you should give in the \n",
    "models = list(SUPPORTED_LLM_MODELS)\n",
    "\n",
    "print(models)\n",
    "\n",
    "models_eval = widgets.Dropdown(\n",
    "    options=models,\n",
    "    value=models[0],\n",
    "    description=\"Models Available: \",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "models_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42c6eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.translate.chrf_score import corpus_chrf\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def evaluate_model(model, tokenizer, input_text, reference_texts):\n",
    "    # Time stamp \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Running the model with the sample input text.  \n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "    output_ids = model.generate(input_ids=input_ids, max_new_tokens=128)\n",
    "    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Latency calculation\n",
    "    latency = (time.time() - start_time) * 1000\n",
    "    num_tokens = len(output_ids[0])\n",
    "\n",
    "    # Throughput calculation\n",
    "    throughput = num_tokens / (latency / 1000)\n",
    "    \n",
    "    # Rouge Score Calcuator\n",
    "    scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
    "    rouge_scores = scorer.score(reference_texts, generated_text)\n",
    "    \n",
    "    # BLEU Score Calculator\n",
    "    smoothing = SmoothingFunction().method1\n",
    "    bleu_score = sentence_bleu([reference_texts.split()], generated_text.split(), smoothing_function=smoothing)\n",
    "    \n",
    "    # chrf Score Calculator\n",
    "    chrf_score = corpus_chrf([[reference_texts]], [[generated_text]])\n",
    "    \n",
    "    # Unique ngrams, entropy and repeated ngrams Calculation\n",
    "    tokens = generated_text.split()\n",
    "    unique_ngrams = len(set(zip(tokens, tokens[1:]))) / len(tokens) if len(tokens) > 1 else 0\n",
    "    entropy = -np.sum([p * np.log2(p) for p in np.unique(tokens, return_counts=True)[1] / len(tokens)])\n",
    "    repeated_ngrams = sum([1 for i in range(len(tokens) - 1) if tokens[i] == tokens[i + 1]]) / len(tokens)\n",
    "    \n",
    "    try:\n",
    "        ref_emb = model.get_input_embeddings()(input_ids).detach().numpy()\n",
    "        gen_emb = model.get_input_embeddings()(output_ids).detach().numpy()\n",
    "\n",
    "        #Coherence Calculator.\n",
    "        coherence = cosine_similarity(ref_emb.mean(axis=1), gen_emb.mean(axis=1))[0][0]\n",
    "    except Exception:\n",
    "        coherence = 0\n",
    "    \n",
    "    return {\n",
    "        \"Model\": f\"{model.config.name_or_path}_{compression_dir}\",\n",
    "        \"Latency (ms)\": latency,\n",
    "        \"Throughput (tokens/sec)\": throughput,\n",
    "        \"ROUGE-1\": rouge_scores[\"rouge1\"].fmeasure,\n",
    "        \"ROUGE-2\": rouge_scores[\"rouge2\"].fmeasure,\n",
    "        \"ROUGE-L\": rouge_scores[\"rougeL\"].fmeasure,\n",
    "        \"BLEU Score\": bleu_score,\n",
    "        \"CHRF Score\": chrf_score,\n",
    "        \"Unique n-grams\": unique_ngrams,\n",
    "        \"Entropy\": entropy,\n",
    "        \"Repeated n-grams (%)\": repeated_ngrams * 100,\n",
    "        \"Coherence (Cosine Similarity)\": coherence,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144007ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from transformers import AutoConfig, AutoTokenizer\n",
    "from optimum.intel.openvino import OVModelForCausalLM\n",
    "import openvino.properties as props\n",
    "import openvino.properties.hint as hints\n",
    "import openvino.properties.streams as streams\n",
    "\n",
    "def evaluate_models_and_save(SUPPORTED_LLM_MODELS, compression_dir):\n",
    "\n",
    "    output_csv=f\"evaluation_results_{compression_dir}.csv\"\n",
    "\n",
    "    # results stored as a list\n",
    "    all_results = []\n",
    "    existing_df = None\n",
    "    \n",
    "    # check if the file already exists, to append to the file and not accidentally create new file everytime and overwrite over it.\n",
    "    if Path(output_csv).exists():\n",
    "        existing_df = pd.read_csv(output_csv)\n",
    "    \n",
    "    model_name = models_eval.value\n",
    "    model_configuration = SUPPORTED_LLM_MODELS[model_name]    \n",
    "    print(f\"Processing model: {model_name}\")\n",
    "    \n",
    "    #loads only the specified precision model file. \n",
    "    model_dir = Path(model_name) / compression_dir  \n",
    "    # only loads the openvino format model. \n",
    "    if not (model_dir / \"openvino_model.xml\").exists():\n",
    "        return\n",
    "    \n",
    "    print(f\"Loading model from {model_dir}\")\n",
    "    ov_config = {\n",
    "        hints.performance_mode(): hints.PerformanceMode.THROUGHPUT,\n",
    "        streams.num(): \"AUTO\",\n",
    "        props.cache_dir(): \"ov_cache\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\n",
    "        model = OVModelForCausalLM.from_pretrained(\n",
    "            model_dir,\n",
    "            device=\"CPU\", # u can set the device here. GPU if u have. \n",
    "            ov_config=ov_config,\n",
    "            config=AutoConfig.from_pretrained(model_dir, trust_remote_code=True),\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {model_name}: {e}\")\n",
    "        return\n",
    "    \n",
    "    # sample input text for testing. \n",
    "    input_text = \"2 + 2 =\"\n",
    "    # one-shot testing (simple one shot)\n",
    "    reference_texts = \"2 + 2 = 4\"\n",
    "    \n",
    "    try:\n",
    "        results = evaluate_model(model, tokenizer, input_text, reference_texts)\n",
    "        results[\"Model\"] = model_name\n",
    "        all_results.append(results)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during evaluation of {model_name}: {e}\")\n",
    "        \n",
    "    # Deletes the evaluated model and its assigned tokenizer to save RAM memory. \n",
    "    del model, tokenizer\n",
    "    # Initializes the Garbage collector to free up memory.\n",
    "    gc.collect()\n",
    "    \n",
    "    # Adding the results to a dataframe for easier analysis.\n",
    "    df = pd.DataFrame(all_results)\n",
    "    if existing_df is not None:\n",
    "        df = pd.concat([existing_df, df], ignore_index=True)\n",
    "    \n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"Results appended to {output_csv}\")\n",
    "\n",
    "compression_dirs = [\"FP16\", \"INT8\", \"INT4\"]\n",
    "# make sure to set the compression_dir vairable\n",
    "# compression_dir = model_compressed_directory_name \n",
    "# Example, compression_dir = FP16 (if the directory is named FP16 in the format, model/FP16/the compressed files.)\n",
    "for compression_dir in compression_dirs:\n",
    "    evaluate_models_and_save(SUPPORTED_LLM_MODELS, compression_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
